# Appendix A: Assessment Tools



These tools are designed to be used quickly in executive workshops or quarterly planning. They are not academic. They are meant to create a shared baseline.



## A1. Data Operations Maturity Checklist



Score each item from 0–2:

**0 = Not in place**

**1 = Partially in place**

**2 = Fully in place**



| Dimension | 0 | 1 | 2 |

|---|---|---|---|

| Data ownership defined by domain | | | |

| Centralized data catalog exists | | | |

| Data quality checks at ingestion | | | |

| Model governance policy documented | | | |

| Cross‑functional data council meets regularly | | | |

| AI use cases tied to business KPIs | | | |

| Embedded analysts in business units | | | |

| Data product roadmap with owners | | | |

| Incident response for data/AI issues | | | |

| Documentation and lineage for key datasets | | | |



**Interpretation:**

**0–8**: Foundational gaps; focus on governance and ownership.

**9–14**: Partial maturity; fix quality and operating cadence.

**15–20**: Ready to scale; focus on productization and sovereignty.



## A2. Governance Readiness Score



Answer yes/no. Each “yes” = 1 point.



Do you have a named executive accountable for data strategy?

Are data access rules documented and enforced?

Is there a defined risk tiering model for AI use cases?

Are model changes tracked and auditable?

Do you have a cross‑functional review board for sensitive AI decisions?

Is there a documented escalation path for data incidents?

Are quality thresholds defined for critical datasets?

Are third‑party AI vendors required to meet documentation standards?



**Interpretation:**

**0–3**: High exposure

**4–6**: Moderate exposure

**7–8**: Governance‑ready



## A3. Data Quality Quick Scan (30‑Minute)



Pick one high‑impact dataset and answer:



**Accuracy:** Do we have ground truth to validate it?

**Completeness:** What percentage of records are missing key fields?

**Consistency:** Do definitions match across systems?

**Timeliness:** How old is the data when it’s used?

**Validity:** Are formats and ranges enforced?

**Relevance:** Is the data actually fit for the use case?

**Uniqueness:** How many duplicates exist?



If you cannot answer at least four of these questions with numbers, the dataset is not AI‑ready.



## A4. Build‑Buy‑Partner Decision Snapshot



Score each statement from 1–5:

This capability is core to our competitive advantage.

We need this in production within 6–12 months.

We have internal expertise to own it long‑term.

We can tolerate vendor dependency.

This will be reused across multiple business units.



**Interpretation:**

**High core + high reuse + low tolerance for dependency:** Build

**Speed critical + low core value:** Buy

**Need speed + need learning:** Partner



## Sources (Draft)

Format: [SRC-##] Claim. Source. Status.

[SRC-01] Assessment tools are original frameworks derived from book content. Internal.
